{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_zNKoGC9Um6"
   },
   "source": [
    "## 1.1 Preliminiary\n",
    "##### Mount drive, import images, train/valid/test-set split, save data locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1727,
     "status": "ok",
     "timestamp": 1608273394151,
     "user": {
      "displayName": "Hyuk Joon Kwon",
      "photoUrl": "",
      "userId": "09699940390854570602"
     },
     "user_tz": 300
    },
    "id": "uWdr6O8LF5tp",
    "outputId": "de557177-5d97-457f-b67e-60895577aa87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving directory:  /content/drive/MyDrive/Applied_Deep_Learning_Project/save_data_345_150_200/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Initialization\n",
    "lowest_level = 3\n",
    "highest_level = 5\n",
    "\n",
    "save_dir_name = 'Lev_345_split_933_stride_150_pmin_50'\n",
    "dir = '/content/drive/MyDrive/Applied_Deep_Learning_Project/'\n",
    "saving_dir = os.path.join(dir,save_dir_name)\n",
    "\n",
    "print(\"saving directory: \", saving_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3970,
     "status": "ok",
     "timestamp": 1608273396402,
     "user": {
      "displayName": "Hyuk Joon Kwon",
      "photoUrl": "",
      "userId": "09699940390854570602"
     },
     "user_tz": 300
    },
    "id": "E_NxUDkQ9TsD",
    "outputId": "77e9a2be-46e3-4b9e-8320-e65704b597e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "\n",
      "TF version:  2.4.0\n",
      "\n",
      "Found GPU at: /device:GPU:0\n",
      "name, driver_version, memory.total [MiB]\n",
      "Tesla P100-PCIE-16GB, 418.67, 16280 MiB\n"
     ]
    }
   ],
   "source": [
    "# Mount drive\n",
    "from google.colab import drive \n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "print()\n",
    "\n",
    "# Check TensorFlow version\n",
    "import tensorflow as tf \n",
    "print(\"TF version: \", tf.__version__)\n",
    "print()\n",
    "\n",
    "# Check which GPU with available RAM\n",
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "\n",
    "!nvidia-smi --query-gpu=gpu_name,driver_version,memory.total --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5053,
     "status": "ok",
     "timestamp": 1608273397491,
     "user": {
      "displayName": "Hyuk Joon Kwon",
      "photoUrl": "",
      "userId": "09699940390854570602"
     },
     "user_tz": 300
    },
    "id": "wFtertID9FwP",
    "outputId": "42360246-fafe-4db9-84b1-88b5c586ea27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: dpkg was interrupted, you must manually run 'dpkg --configure -a' to correct the problem. \n",
      "E: dpkg was interrupted, you must manually run 'dpkg --configure -a' to correct the problem. \n"
     ]
    }
   ],
   "source": [
    "!apt-get install openslide-tools  # Openslide , Install the OpenSlide C library and Python bindings\n",
    "!apt-get install python3-openslide # After installing these libraries, use `Runtime -> restart and run all` on the menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WWTPFoDTx1Qm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from openslide import open_slide, __library_version__ as openslide_version\n",
    "import os\n",
    "# from PIL import Image\n",
    "from skimage.color import rgb2gray\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import chain\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.layers import Dense, Input, GlobalAveragePooling2D, concatenate\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from matplotlib.patches import Rectangle\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x6CeC7Zf9azt"
   },
   "outputs": [],
   "source": [
    "def get_open_slide(tissue_path, mask_path, return_tissue):\n",
    "# Function that opens an image based on path\n",
    "# Note: return_tissue = boolean value, \n",
    "#      Ture -> reads tissue img, False -> reads masks image\n",
    "\n",
    "  slide = open_slide(tissue_path)\n",
    "  print (\"Read WSI from %s with width: %d, height: %d\" % (tissue_path, \n",
    "                                                          slide.level_dimensions[0][0], \n",
    "                                                          slide.level_dimensions[0][1]))\n",
    "  mask = open_slide(mask_path)\n",
    "  print (\"Read tumor mask from %s\" % (mask_path))\n",
    "\n",
    "  print(\"Slide includes %d levels\", min(len(slide.level_dimensions),len(mask.level_dimensions)))\n",
    "  for i in range(min(len(slide.level_dimensions),len(mask.level_dimensions))):\n",
    "      print(\"Level %d, dimensions: %s downsample factor %d\" % (i, \n",
    "                                                              slide.level_dimensions[i], \n",
    "                                                              slide.level_downsamples[i]))\n",
    "\n",
    "      assert mask.level_dimensions[i][0] == slide.level_dimensions[i][0]\n",
    "      assert mask.level_dimensions[i][1] == slide.level_dimensions[i][1]\n",
    "\n",
    "  # Verify downsampling works as expected\n",
    "  width, height = slide.level_dimensions[7]\n",
    "  assert width * slide.level_downsamples[7] == slide.level_dimensions[0][0]\n",
    "  assert height * slide.level_downsamples[7] == slide.level_dimensions[0][1]\n",
    "\n",
    "  if (return_tissue):\n",
    "    return slide\n",
    "  else:\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aCh3HFAk9g6F"
   },
   "outputs": [],
   "source": [
    "# See https://openslide.org/api/python/#openslide.OpenSlide.read_region\n",
    "# Note: x,y coords are with respect to level 0. \n",
    "# There is an example below of working with coordinates with respect to a higher zoom level.\n",
    "\n",
    "# Read a region from the slide, Return a numpy RBG array\n",
    "def read_slide(slide, x, y, level, width, height, as_float=False):\n",
    "    im = slide.read_region((x,y), level, (width, height))\n",
    "    im = im.convert('RGB') # drop the alpha channel\n",
    "    if as_float:\n",
    "        im = np.asarray(im, dtype=np.float32)\n",
    "    else:\n",
    "        im = np.asarray(im)\n",
    "    assert im.shape == (height, width, 3)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uoip8-z8ya5U"
   },
   "outputs": [],
   "source": [
    "# Gets difference of two lists using set operator\n",
    "def difference(L1, L2):\n",
    "    return (list(list(set(L1)-set(L2)) + list(set(L2)-set(L1))))\n",
    "\n",
    "# detect and remove if number of images and masks are different\n",
    "def balance_imgs(image_path_l, mask_path_l):\n",
    "  img_num_l = [re.findall(r'\\d+', string)[0] for string in image_path_l ] # list of image numbers\n",
    "  mask_num_l = [re.findall(r'\\d+', string)[0] for string in mask_path_l ] # list of mask's image numbers\n",
    "\n",
    "  img_len = len(img_num_l)\n",
    "  mask_len = len(mask_num_l)\n",
    "\n",
    "  # delete images that are not same in number of lengths automatically\n",
    "  if img_len != mask_len:\n",
    "    print(\"Tissue image length: {}\".format(len(image_path_l)))\n",
    "    print(\"Mask image length: {}\".format(len(mask_path_l)))\n",
    "\n",
    "    diff_img_num = difference(img_num_l,mask_num_l)[0]\n",
    "\n",
    "    if img_len > mask_len: # i.e. need to find extra image and delete\n",
    "      print(\"Removed image number {} since there is no corresponding biopsy image\".format(diff_img_num))\n",
    "      del image_path_l[img_num_l.index(diff_img_num)]\n",
    "\n",
    "    else: \n",
    "      print(\"Removed mask number {} since there is no corresponding  image\".format(diff_img_num))\n",
    "      del mask_path_l[mask_num_l.index(diff_img_num)]\n",
    "\n",
    "  return image_path_l, mask_path_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6135,
     "status": "ok",
     "timestamp": 1608273398591,
     "user": {
      "displayName": "Hyuk Joon Kwon",
      "photoUrl": "",
      "userId": "09699940390854570602"
     },
     "user_tz": 300
    },
    "id": "3zvmW3bfyemc",
    "outputId": "7f072e98-db25-4776-b6fa-1e137846b9aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tissue image length: 22\n",
      "Mask image length: 21\n",
      "Removed image number 038 since there is no corresponding biopsy image\n"
     ]
    }
   ],
   "source": [
    "# Note: since importing slides from Professor's Google API does not work sometimes,\n",
    "# we downloaded the images to 'locally' in Google Drive\n",
    "directory = '/content/drive/MyDrive/Applied_Deep_Learning_Project/slides_local/'\n",
    "\n",
    "biopsy_path_list = []\n",
    "mask_path_list = []\n",
    "\n",
    "for file in os.scandir(directory):\n",
    "  \n",
    "  if (\"tif\" in file.name):\n",
    "    if (\"mask\" in file.name):\n",
    "      mask_path_list.append(file.path) \n",
    "  \n",
    "    else:\n",
    "      biopsy_path_list.append(file.path)\n",
    "\n",
    "# # Automatically removes images that are not both exists in image and masks\n",
    "biopsy_path_list,mask_path_list = balance_imgs(biopsy_path_list,mask_path_list)\n",
    "\n",
    "biopsy_path_list.sort()\n",
    "mask_path_list.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_v5SW89zeEn"
   },
   "source": [
    "####  Since the number of biopsy and mask images are different, image number 38 got deleted from the initially given sample. Also, we removed some other biopsy/mask images that do not have cancer percentage high enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AZSBSo79-xC1"
   },
   "outputs": [],
   "source": [
    "def extract_img_numbers(string_list):\n",
    "  int_list = []\n",
    "  for str in string_list:\n",
    "    img_num = re.findall(r'\\d+', str)[0]\n",
    "    int_list.append(int(img_num))\n",
    "\n",
    "  return sorted(int_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrG0L0sCAWaN"
   },
   "source": [
    "### Perhaps show initial analysis? (i.e. pictures of biopsy, mask image?)\n",
    "#### i.e. what is training biopsy image looks like, valid, test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9HGz35Y-zJT"
   },
   "outputs": [],
   "source": [
    "# %pprint\n",
    "# biopsy_img_num_l = extract_img_numbers(biopsy_path_list)\n",
    "# display(biopsy_img_num_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zH_9ZXY0-qQJ"
   },
   "outputs": [],
   "source": [
    "train_index = [2,4,5,6,7,12,16,18,20]\n",
    "test_index = [13]\n",
    "valid_index = [0,15,19]\n",
    "\n",
    "biopsy_train = [biopsy_path_list[i] for i in train_index]\n",
    "biopsy_test = [biopsy_path_list[i] for i in test_index]\n",
    "biopsy_valid = [biopsy_path_list[i] for i in valid_index]\n",
    "mask_train = [mask_path_list[i] for i in train_index]\n",
    "mask_test = [mask_path_list[i] for i in test_index]\n",
    "mask_valid = [mask_path_list[i] for i in valid_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6457,
     "status": "ok",
     "timestamp": 1608273398926,
     "user": {
      "displayName": "Hyuk Joon Kwon",
      "photoUrl": "",
      "userId": "09699940390854570602"
     },
     "user_tz": 300
    },
    "id": "vT75LDYv_7Ee",
    "outputId": "cabb7a0c-5647-4c26-a65b-325520d53337"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num of samples used:  13\n",
      "Each train/val/test set length: \n",
      " biopsy -> train: 9 val: 3 test: 1 \n",
      " Mask -> train: 9 val: 3 test: 1\n",
      "\n",
      "Biopsy img number in train set: [5, 16, 19, 23, 31, 75, 91, 96, 110]\n",
      "Biopsy img number in valid set: [1, 84, 101]\n",
      "Biopsy img number in test set: [78]\n"
     ]
    }
   ],
   "source": [
    "print(\"Total num of samples used: \", len(biopsy_train) + len(biopsy_valid) +len(biopsy_test))\n",
    "print(\"Each train/val/test set length: \\n biopsy -> train: {} val: {} test: {} \\n Mask -> train: {} val: {} test: {}\".format(\n",
    "     len(biopsy_train), len(biopsy_valid), len(biopsy_test), \n",
    "     len(mask_train), len(mask_valid), len(mask_test)  ))\n",
    "\n",
    "print()\n",
    "print(\"Biopsy img number in train set: {}\".format(extract_img_numbers(biopsy_train)))\n",
    "print(\"Biopsy img number in valid set: {}\".format(extract_img_numbers(biopsy_valid)))\n",
    "print(\"Biopsy img number in test set: {}\".format(extract_img_numbers(biopsy_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14023,
     "status": "ok",
     "timestamp": 1608273406496,
     "user": {
      "displayName": "Hyuk Joon Kwon",
      "photoUrl": "",
      "userId": "09699940390854570602"
     },
     "user_tz": 300
    },
    "id": "D595-G-g-jYT",
    "outputId": "cb92bbf0-fdee-43c6-f734-3796a3698345"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read WSI from /content/drive/MyDrive/Applied_Deep_Learning_Project/slides_local/tumor_078.tif with width: 94208, height: 111104\n",
      "Read tumor mask from /content/drive/MyDrive/Applied_Deep_Learning_Project/slides_local/tumor_078_mask.tif\n",
      "Slide includes %d levels 9\n",
      "Level 0, dimensions: (94208, 111104) downsample factor 1\n",
      "Level 1, dimensions: (47104, 55552) downsample factor 2\n",
      "Level 2, dimensions: (23552, 27776) downsample factor 4\n",
      "Level 3, dimensions: (11776, 13888) downsample factor 8\n",
      "Level 4, dimensions: (5888, 6944) downsample factor 16\n",
      "Level 5, dimensions: (2944, 3472) downsample factor 32\n",
      "Level 6, dimensions: (1472, 1736) downsample factor 64\n",
      "Level 7, dimensions: (736, 868) downsample factor 128\n",
      "Level 8, dimensions: (368, 434) downsample factor 256\n",
      "Read WSI from /content/drive/MyDrive/Applied_Deep_Learning_Project/slides_local/tumor_078.tif with width: 94208, height: 111104\n",
      "Read tumor mask from /content/drive/MyDrive/Applied_Deep_Learning_Project/slides_local/tumor_078_mask.tif\n",
      "Slide includes %d levels 9\n",
      "Level 0, dimensions: (94208, 111104) downsample factor 1\n",
      "Level 1, dimensions: (47104, 55552) downsample factor 2\n",
      "Level 2, dimensions: (23552, 27776) downsample factor 4\n",
      "Level 3, dimensions: (11776, 13888) downsample factor 8\n",
      "Level 4, dimensions: (5888, 6944) downsample factor 16\n",
      "Level 5, dimensions: (2944, 3472) downsample factor 32\n",
      "Level 6, dimensions: (1472, 1736) downsample factor 64\n",
      "Level 7, dimensions: (736, 868) downsample factor 128\n",
      "Level 8, dimensions: (368, 434) downsample factor 256\n"
     ]
    }
   ],
   "source": [
    "#Getting the slides using each of the train/test/valid directory\n",
    "# biopsy_slides_train = [get_open_slide(x,y,True) for x,y in zip(biopsy_train, mask_train)] # Ture -> reads tumor img, False -> reads masks image\n",
    "# mask_slides_train = [get_open_slide(x,y,False) for x,y in zip(biopsy_train, mask_train)]\n",
    "\n",
    "# biopsy_slides_valid = [get_open_slide(x,y,True) for x,y in zip(biopsy_valid, mask_valid)]\n",
    "# mask_slides_valid = [get_open_slide(x,y,False) for x,y in zip(biopsy_valid, mask_valid)]\n",
    "\n",
    "biopsy_slides_test = [get_open_slide(x,y,True) for x,y in zip(biopsy_test, mask_test)]\n",
    "mask_slides_test = [get_open_slide(x,y,False) for x,y in zip(biopsy_test, mask_test)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVbZ0Z9lxozc"
   },
   "source": [
    "#### Use filter to facilitate training proces\n",
    "##### i.e. regions where tissue is -> where grey value is lower than 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OoiysdHsxoap"
   },
   "outputs": [],
   "source": [
    "# As mentioned in class, we can improve efficiency by ignoring non-tissue areas \n",
    "# of the slide. We'll find these by looking for all gray regions.\n",
    "def find_tissue_pixels(image, intensity=0.8):\n",
    "    im_gray = rgb2gray(image)\n",
    "    assert im_gray.shape == (image.shape[0], image.shape[1])\n",
    "    indices = np.where(im_gray <= intensity)\n",
    "    return len(indices[0]) # i.e. return length of number of pixels that have lower intensity\n",
    "\n",
    "# A modified version of the finding tissue pixel \n",
    "# Returns True if area of present biopsy image is above the threshold, min_percentage,\n",
    "def pass_tissue_percentage(slide_image, tissue_p_min, w, h):\n",
    "  tissue_pixels = find_tissue_pixels(slide_image)\n",
    "  percent_tissue = tissue_pixels/ float(w*h) * 100\n",
    "\n",
    "  if percent_tissue > tissue_p_min:\n",
    "    # print(\"----------------\",(\"%.2f\" % round(percent_tissue, 2)))\n",
    "    return True\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "# test whether a patch has cancer or not\n",
    "def has_cancer(patch):\n",
    "  return sum(sum(chain.from_iterable(patch))) != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7xCxrTuWsT9S"
   },
   "outputs": [],
   "source": [
    "def downsample_factor(slide, level):\n",
    "\n",
    "  return (int(slide.level_downsamples[level]))\n",
    "\n",
    "\n",
    "# Transforms a coordinate associated with a current zoom level\n",
    "def transform_coord(slide, input_coord, base_level, current_level, patch_size):\n",
    "\n",
    "  base_lev_factor = downsample_factor(slide,base_level)\n",
    "  current_lev_factor = downsample_factor(slide,current_level)\n",
    "\n",
    "  new_coord = input_coord * base_lev_factor - int(patch_size/2) * current_lev_factor\n",
    "  return new_coord\n",
    "\n",
    "\n",
    "# Reads slide according to the transformed coordinate\n",
    "def read_zoom_slide(slide, x_input, y_input, base_lev, current_lev, patch_size):\n",
    "\n",
    "  slide = read_slide(slide, \n",
    "                     transform_coord(slide, x_input, base_lev, current_lev, patch_size ),\n",
    "                     transform_coord(slide, y_input, base_lev, current_lev, patch_size ),\n",
    "                     level = current_lev, width = patch_size, height = patch_size)\n",
    "  return slide\n",
    "\n",
    "# Computes a relative magnification difference between two zoom levels\n",
    "def compute_relative_mag(base_lev, current_lev):\n",
    "\n",
    "  result = 2 ** (base_lev - current_lev)  # i.e. base_lev = 7, current_lev =5, then current level is zoomed 4 times more\n",
    "  return result\n",
    "\n",
    "\n",
    "# Computes Rectangle with appropriate coordinates in the current zoom level,\n",
    "# with relative to the base level (most zoomed out level) \n",
    "def compute_rectangle(relative_magnification, patch_size = 299, center_size = 128):\n",
    "\n",
    "  return Rectangle((int(patch_size / 2) - int(center_size / (2 ** (relative_magnification + 1))),\n",
    "                  int(patch_size / 2) - int(center_size / (2 ** (relative_magnification + 1)))),\n",
    "                  center_size / (2**relative_magnification),\n",
    "                  center_size  / (2**relative_magnification),\n",
    "                  facecolor='none', edgecolor='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wY7czqw50XX1"
   },
   "source": [
    "### Testing whether making center patch coordinates works for different zoom levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JdG-qix2sOlX"
   },
   "outputs": [],
   "source": [
    "# index = 2\n",
    "# base_level = 4\n",
    "# biopsy_slide = biopsy_slides_train[index]\n",
    "# mask_slide = mask_slides_train[index]\n",
    "\n",
    "# downsample_f = downsample_factor(biopsy_slide, base_level)\n",
    "\n",
    "# biopsy_img = read_slide(biopsy_slide, x=0, y=0, level=base_level, \n",
    "#                         width=biopsy_slide.level_dimensions[base_level][0], height=biopsy_slide.level_dimensions[base_level][1])\n",
    "# mask_img = read_slide(mask_slide, x=0, y=0, level = base_level, \n",
    "#                          width=mask_slide.level_dimensions[base_level][0], height=mask_slide.level_dimensions[base_level][1])\n",
    "\n",
    "# fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,10))\n",
    "# axes[0].imshow(biopsy_img)\n",
    "# axes[0].set_title(\"Original Biopsy Image\")\n",
    "# axes[1].imshow(mask_img[:,:,0])\n",
    "# axes[1].set_title(\"Original Mask Image\")\n",
    "\n",
    "# p_size =299 # patch size\n",
    "# x = 150 # starting patch position\n",
    "# y =150\n",
    "\n",
    "# lev_7 = base_level\n",
    "# lev_6 =6\n",
    "# lev_5 = 5\n",
    "\n",
    "# slide_image_7 = read_zoom_slide(biopsy_slide, x, y, base_level, lev_7, p_size)\n",
    "# slide_image_6 = read_zoom_slide(biopsy_slide, x, y, base_level, lev_6, p_size)\n",
    "# slide_image_5 = read_zoom_slide(biopsy_slide, x, y, base_level, lev_5, p_size)\n",
    "\n",
    "# fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15,10))\n",
    "# fig.suptitle(\"Testing center region in patches with different zoom levels\", fontsize=16)\n",
    "# fig.subplots_adjust(top=1.45)\n",
    "\n",
    "# axes[0].imshow(slide_image_5)\n",
    "# axes[0].add_patch(Rectangle((150-128//2,150-128//2),128,128,facecolor='none',edgecolor='red'))\n",
    "# axes[1].imshow(slide_image_6)\n",
    "# axes[1].add_patch(compute_rectangle(1))\n",
    "# axes[2].imshow(slide_image_7)\n",
    "# axes[2].add_patch(compute_rectangle(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BzqpP5_cMk-3"
   },
   "outputs": [],
   "source": [
    "def print_patches(patch_list, base_l, current_l):\n",
    "  number_patches = len(patch_list)\n",
    "  fig, axes = plt.subplots(nrows=1, ncols=number_patches, figsize=(15,10))\n",
    "  for idx, slide in enumerate(patch_list):\n",
    "    axes[idx].imshow(slide)\n",
    "    axes[idx].add_patch(compute_rectangle(idx))\n",
    "\n",
    "def print_masks(patch_list, base_l, current_l):\n",
    "  number_patches = len(patch_list)\n",
    "  fig, axes = plt.subplots(nrows=1, ncols=number_patches, figsize=(15,10))\n",
    "  for idx, slide in enumerate(patch_list):\n",
    "    axes[idx].imshow(slide[:,:,0])\n",
    "    axes[idx].add_patch(compute_rectangle(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3N1BAAfND0fB"
   },
   "outputs": [],
   "source": [
    "def create_patches(biopsy_slide, mask_slide, lowest_level = 5, highest_level = 7, apply_filter = False):\n",
    "\n",
    "  p_size = 299  # patch size\n",
    "  stride = 299  # need to chnage (here, for zoom level 5, which is lowest level)\n",
    "\n",
    "  tissue_p_min = 40  # i.e. treshold of minimum percentage of tissue converaged in percentage\n",
    "                # i.e. if patch has more than 10% of tissue pixels, then take it\n",
    "  biopsy_list = []\n",
    "  label_list = []\n",
    "\n",
    "  # Starting center coordinates\n",
    "  x = int(p_size/2) # i.e. 150\n",
    "  y = int(p_size/2)\n",
    "\n",
    "  biopsy_w_high = biopsy_slide.level_dimensions[highest_level][0]\n",
    "  biopsy_h_high = biopsy_slide.level_dimensions[highest_level][1]\n",
    "\n",
    "  biopsy_w_low = biopsy_slide.level_dimensions[lowest_level][0]\n",
    "  biopsy_h_low = biopsy_slide.level_dimensions[lowest_level][1]\n",
    "\n",
    "  # print(biopsy_w_high, biopsy_w_high * slide.level_downsamples[base])\n",
    "\n",
    "  biopsy_list_batch = []\n",
    "  \n",
    "  run = True\n",
    "\n",
    "  global patch_counter\n",
    "  patch_counter=0\n",
    "  patch_x_counter=0\n",
    "  patch_y_counter=0\n",
    "\n",
    "  print(\"lowest dim shape:\",biopsy_w_low, \" \", biopsy_h_low )\n",
    "\n",
    "  # print(\"Applying filtering? : -> \", apply_filter)\n",
    "\n",
    "  while(run):\n",
    "    # print(\"Coordinate: \" + str(x) + \" \" + str(y))\n",
    "     \n",
    "\n",
    "    if (y < biopsy_h_high  - 299//2):  # i.e. slide based on highest level since it is most zoomed out version\n",
    "      if (x < biopsy_w_high - 299//2):\n",
    "        biopsy_list_batch = []\n",
    "\n",
    "        slide_image_temp = read_zoom_slide(biopsy_slide, x, y, highest_level, lowest_level, p_size)\n",
    "\n",
    "        if apply_filter: # i.e. apply filtering only for training set, s\n",
    "          # print(\"Applying tissue filtering\")\n",
    "          if pass_tissue_percentage(slide_image_temp, tissue_p_min, p_size, p_size): # i.e. only using if lowest label passes the tissue test\n",
    "\n",
    "            for current_level in range(lowest_level, highest_level + 1):\n",
    "              slide_image = read_zoom_slide(biopsy_slide, x, y, highest_level, current_level, p_size)\n",
    "              mask_image = read_zoom_slide(mask_slide, x, y, highest_level, current_level, p_size)\n",
    "\n",
    "              biopsy_list_batch.append(slide_image)\n",
    "\n",
    "              mask_image = read_zoom_slide(mask_slide, x, y, highest_level, current_level, p_size)\n",
    "              if (current_level == lowest_level): # Do it with only the first image\n",
    "                # print(\"pass_cord: \" + str(x) + \" \" + str(y) + \" \" + str(has_cancer(mask_image)), end=', ', flush=True)\n",
    "                label_list.append(has_cancer(mask_image))\n",
    "                                        \n",
    "              else:\n",
    "                continue;\n",
    "\n",
    "        else:  # do not apply tissue filtering for valid, test set\n",
    "          # print(\"Not applying filtering for training set\")\n",
    "          for current_level in range(lowest_level, highest_level + 1):\n",
    "            slide_image = read_zoom_slide(biopsy_slide, x, y, highest_level, current_level, p_size)\n",
    "            mask_image = read_zoom_slide(mask_slide, x, y, highest_level, current_level, p_size)\n",
    "\n",
    "            biopsy_list_batch.append(slide_image)\n",
    "\n",
    "            mask_image = read_zoom_slide(mask_slide, x, y, highest_level, current_level, p_size)\n",
    "            if (current_level == lowest_level): # Do it with only the first image\n",
    "              # print(\"pass_cord: \" + str(x) + \" \" + str(y) + \" \" + str(has_cancer(mask_image)), end=', ', flush=True)\n",
    "              label_list.append(has_cancer(mask_image))\n",
    "                                      \n",
    "            else:\n",
    "              continue;\n",
    "        # print_patches(biopsy_list_batch, highest_level, current_level)\n",
    "        # print_masks(mask_list_batch, highest_level, current_level)\n",
    "\n",
    "        if len(biopsy_list_batch) !=0:\n",
    "          biopsy_list.append(biopsy_list_batch)\n",
    "\n",
    "        patch_counter += 1\n",
    "        patch_x_counter += 1\n",
    "        x = x + int(stride / biopsy_slide.level_downsamples[highest_level] *  biopsy_slide.level_downsamples[lowest_level])\n",
    "        \n",
    "      else: # i.e. progress toward next 'row' below\n",
    "        x = int(p_size/2) \n",
    "        y = y + int(stride / biopsy_slide.level_downsamples[highest_level] *  biopsy_slide.level_downsamples[lowest_level])\n",
    "      \n",
    "        patch_y_counter += 1\n",
    "        if (y < biopsy_h_high  - 299//2):\n",
    "          patch_x_counter = 0\n",
    "\n",
    "    else: \n",
    "      run = False  \n",
    "  \n",
    "\n",
    "  assert len(biopsy_list) == len(label_list)\n",
    "  print(\"tissue filtering percentage used: \", \"{:.2f}\".format(len(biopsy_list)/patch_counter*100), \"%\")\n",
    "\n",
    "  if apply_filter: # i.e. for training set\n",
    "    return biopsy_list, label_list\n",
    "  \n",
    "  else: # i.e. for test set, also returning patches_num\n",
    "    patches_num = [patch_counter, patch_x_counter, patch_y_counter]\n",
    "    print(\"patches num: \", patches_num)\n",
    "    \n",
    "    return biopsy_list, label_list, patches_num \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d249BrB9kg5q"
   },
   "source": [
    "### Flatten and put train and validation set into Numpy array formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Is4BpB-vkFHM"
   },
   "outputs": [],
   "source": [
    "# flatten each biopsy patch into its zoom level\n",
    "def flatten_biopsy_zoom(biopsy_patch):\n",
    "  zoom1 = []\n",
    "  zoom2 = []\n",
    "  zoom3 = []\n",
    "\n",
    "  for biopsy in biopsy_patch:\n",
    "    # print(np.array(biopsy).shape)\n",
    "    for patch_img in biopsy:\n",
    "      zoom1.append(patch_img[0])\n",
    "      zoom2.append(patch_img[1])\n",
    "      zoom3.append(patch_img[2])\n",
    "\n",
    "  assert np.array(zoom1).shape == np.array(zoom2).shape == np.array(zoom3).shape\n",
    "  return  np.array(zoom1),  np.array(zoom2),  np.array(zoom3)\n",
    "\n",
    "# flatten label\n",
    "def flatten_label(label_list):\n",
    "  label = []\n",
    "  for image in label_list:\n",
    "    for i in image:\n",
    "      label.append(i*1)\n",
    "  return np.array(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6t0lXiS5E6v7"
   },
   "source": [
    "#### Save Train and Validation set as an Numpy format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XvTGi-dc66ws"
   },
   "outputs": [],
   "source": [
    "# files_list = os.listdir(saving_dir)\n",
    "\n",
    "# if len(files_list) != 0:\n",
    "#   for file in files_list: #i.e. delete old files each time\n",
    "#     os.remove(os.path.join(saving_dir,file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 174669,
     "status": "ok",
     "timestamp": 1608273567161,
     "user": {
      "displayName": "Hyuk Joon Kwon",
      "photoUrl": "",
      "userId": "09699940390854570602"
     },
     "user_tz": 300
    },
    "id": "a_9pmvnyGbsS",
    "outputId": "28fa3871-9761-4f14-c831-8f34aece693a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lowest dim shape: 11776   13888\n",
      "tissue filtering percentage used:  100.00 %\n",
      "patches num:  [1548, 36, 43]\n"
     ]
    }
   ],
   "source": [
    "result_test =[create_patches(slide, mask, lowest_level = lowest_level, highest_level = highest_level, apply_filter = False) for slide,mask in zip(biopsy_slides_test, mask_slides_test)]\n",
    "biopsy_patch_test = [item[0] for item in result_test]\n",
    "label_test =[item[1] for item in result_test]\n",
    "patches_num_test = [item[2] for item in result_test] # i.e. list of sublist, where sublist is [patches_num, x_num, y_num]\n",
    "                                                              # patch_counter, patch_x_counter, patch_y_counter\n",
    "\n",
    "zoom1_test, zoom2_test, zoom3_test = flatten_biopsy_zoom(biopsy_patch_test)\n",
    "label_test = flatten_label(label_test)\n",
    "\n",
    "# saving files in Numpy formats\n",
    "np.save(os.path.join(saving_dir,'zoom1_test_13.npy'),zoom1_test)\n",
    "np.save(os.path.join(saving_dir,'zoom2_test_13.npy'),zoom2_test)\n",
    "np.save(os.path.join(saving_dir,'zoom3_test_13.npy'),zoom3_test)\n",
    "np.save(os.path.join(saving_dir,'label_test_13.npy'),label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 174665,
     "status": "ok",
     "timestamp": 1608273567162,
     "user": {
      "displayName": "Hyuk Joon Kwon",
      "photoUrl": "",
      "userId": "09699940390854570602"
     },
     "user_tz": 300
    },
    "id": "_UrRu1IC3T87",
    "outputId": "e9150c4a-61c2-49b6-db01-77c6fa1c5c7d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1548, 36, 43]]"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches_num_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RGV6RYo43V0n"
   },
   "outputs": [],
   "source": [
    "np.save(os.path.join(saving_dir,'patches_num_test_13.npy'),np.array(patches_num_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UeMwKXtGLpUp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "[Leo]1.2.Save_Data_test.ipynb",
   "provenance": [
    {
     "file_id": "1wH2wqmZIGzzNlKQgI45KU6h_HChHuA_C",
     "timestamp": 1607660258274
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
