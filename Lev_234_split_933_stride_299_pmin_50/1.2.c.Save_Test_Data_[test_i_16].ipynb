{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"1.2.c.Save_Test_Data_[test_i_16].ipynb","provenance":[{"file_id":"1wH2wqmZIGzzNlKQgI45KU6h_HChHuA_C","timestamp":1607660258274}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"I_zNKoGC9Um6"},"source":["## 1.1 Preliminiary\n","### Mount drive, import images, test set, save data locally\n","\n","**Used Hyperparameter**<br>\n","Level: 2,3,4 images <br>\n","Number of train,val,test: 9, 3, 3 <br>\n","train's stride : 299 <br>\n","test's stride: 299 <br>\n","tissue percent min: 50 <br>\n","\n","----------------------------------------------<br>\n","train_index = [2,4,5,6,7,15,17,19,20]<br>\n","valid_index = [0,12,18]<br>\n","test_index = [11,13,16]<br>\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CrIymQMy5XKt","executionInfo":{"status":"ok","timestamp":1608274021157,"user_tz":300,"elapsed":765,"user":{"displayName":"Hyuk Joon Kwon","photoUrl":"","userId":"09809686230726823589"}},"outputId":"4688fd83-9aa8-4526-9111-f1e1b8cffd03"},"source":["import os\r\n","\r\n","# Initialization\r\n","lowest_level = 2\r\n","highest_level = 4\r\n","train_val_stride = 299\r\n","test_stride = 299\r\n","tissue_p_min = 50\r\n","\r\n","save_dir_name = 'Lev_234_split_933_stride_299_pmin_50'\r\n","dir = '/content/drive/MyDrive/Applied_Deep_Learning_Project/'\r\n","saving_dir = os.path.join(dir,save_dir_name)\r\n","\r\n","print(\"saving directory: \", saving_dir)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["saving directory:  /content/drive/MyDrive/Applied_Deep_Learning_Project/save_data_234_downsample/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"E_NxUDkQ9TsD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608274024509,"user_tz":300,"elapsed":4110,"user":{"displayName":"Hyuk Joon Kwon","photoUrl":"","userId":"09809686230726823589"}},"outputId":"11c7da3b-23a2-42d7-98eb-83741cdd068b"},"source":["# Mount drive\n","from google.colab import drive \n","drive.mount('/content/drive', force_remount=True)\n","print()\n","\n","# Check TensorFlow version\n","import tensorflow as tf \n","print(\"TF version: \", tf.__version__)\n","print()\n","\n","# Check which GPU with available RAM\n","import tensorflow as tf\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))\n","\n","!nvidia-smi --query-gpu=gpu_name,driver_version,memory.total --format=csv"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","\n","TF version:  2.4.0\n","\n","Found GPU at: /device:GPU:0\n","name, driver_version, memory.total [MiB]\n","Tesla P100-PCIE-16GB, 418.67, 16280 MiB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wFtertID9FwP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608274027885,"user_tz":300,"elapsed":7481,"user":{"displayName":"Hyuk Joon Kwon","photoUrl":"","userId":"09809686230726823589"}},"outputId":"e01e9cc5-76ac-4e77-968c-1528641b1877"},"source":["!apt-get install openslide-tools  # Openslide , Install the OpenSlide C library and Python bindings\n","!apt-get install python3-openslide # After installing these libraries, use `Runtime -> restart and run all` on the menu"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","openslide-tools is already the newest version (3.4.1+dfsg-2).\n","0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","python3-openslide is already the newest version (1.1.1-2ubuntu4).\n","0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WWTPFoDTx1Qm"},"source":["import numpy as np\n","from openslide import open_slide, __library_version__ as openslide_version\n","\n","from openslide import open_slide, __library_version__ as openslide_version\n","import os\n","# from PIL import Image\n","from skimage.color import rgb2gray\n","from sklearn.model_selection import train_test_split\n","from itertools import chain\n","from tensorflow.keras.applications import InceptionV3\n","from tensorflow.keras.layers import Dense, Input, GlobalAveragePooling2D, concatenate\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.utils import plot_model\n","from matplotlib.patches import Rectangle\n","import re\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x6CeC7Zf9azt"},"source":["def get_open_slide(tissue_path, mask_path, return_tissue):\n","# Function that opens an image based on path\n","# Note: return_tissue = boolean value, \n","#      Ture -> reads tissue img, False -> reads masks image\n","\n","  slide = open_slide(tissue_path)\n","  print (\"Read WSI from %s with width: %d, height: %d\" % (tissue_path, \n","                                                          slide.level_dimensions[0][0], \n","                                                          slide.level_dimensions[0][1]))\n","  mask = open_slide(mask_path)\n","  print (\"Read tumor mask from %s\" % (mask_path))\n","\n","  print(\"Slide includes %d levels\", min(len(slide.level_dimensions),len(mask.level_dimensions)))\n","  for i in range(min(len(slide.level_dimensions),len(mask.level_dimensions))):\n","      print(\"Level %d, dimensions: %s downsample factor %d\" % (i, \n","                                                              slide.level_dimensions[i], \n","                                                              slide.level_downsamples[i]))\n","      assert mask.level_dimensions[i][0] == slide.level_dimensions[i][0]\n","      assert mask.level_dimensions[i][1] == slide.level_dimensions[i][1]\n","\n","  # Verify downsampling works as expected\n","  width, height = slide.level_dimensions[7]\n","  assert width * slide.level_downsamples[7] == slide.level_dimensions[0][0]\n","  assert height * slide.level_downsamples[7] == slide.level_dimensions[0][1]\n","\n","  if (return_tissue):\n","    return slide\n","  else:\n","    return mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aCh3HFAk9g6F"},"source":["# See https://openslide.org/api/python/#openslide.OpenSlide.read_region\n","# Note: x,y coords are with respect to level 0. \n","# There is an example below of working with coordinates with respect to a higher zoom level.\n","\n","# Read a region from the slide, Return a numpy RBG array\n","def read_slide(slide, x, y, level, width, height, as_float=False):\n","    im = slide.read_region((x,y), level, (width, height))\n","    im = im.convert('RGB') # drop the alpha channel\n","    if as_float:\n","        im = np.asarray(im, dtype=np.float32)\n","    else:\n","        im = np.asarray(im)\n","    assert im.shape == (height, width, 3)\n","    return im"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uoip8-z8ya5U"},"source":["# Gets difference of two lists using set operator\n","def difference(L1, L2):\n","    return (list(list(set(L1)-set(L2)) + list(set(L2)-set(L1))))\n","\n","# detect and remove if number of images and masks are different\n","def balance_imgs(image_path_l, mask_path_l):\n","  img_num_l = [re.findall(r'\\d+', string)[0] for string in image_path_l ] # list of image numbers\n","  mask_num_l = [re.findall(r'\\d+', string)[0] for string in mask_path_l ] # list of mask's image numbers\n","\n","  img_len = len(img_num_l)\n","  mask_len = len(mask_num_l)\n","\n","  # delete images that are not same in number of lengths automatically\n","  if img_len != mask_len:\n","    print(\"Tissue image length: {}\".format(len(image_path_l)))\n","    print(\"Mask image length: {}\".format(len(mask_path_l)))\n","\n","    diff_img_num = difference(img_num_l,mask_num_l)[0]\n","\n","    if img_len > mask_len: # i.e. need to find extra image and delete\n","      print(\"Removed image number {} since there is no corresponding biopsy image\".format(diff_img_num))\n","      del image_path_l[img_num_l.index(diff_img_num)]\n","\n","    else: \n","      print(\"Removed mask number {} since there is no corresponding  image\".format(diff_img_num))\n","      del mask_path_l[mask_num_l.index(diff_img_num)]\n","\n","  return image_path_l, mask_path_l"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3zvmW3bfyemc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608274028709,"user_tz":300,"elapsed":8283,"user":{"displayName":"Hyuk Joon Kwon","photoUrl":"","userId":"09809686230726823589"}},"outputId":"9be271a7-193e-45ab-9bbc-2e4a3d729a0d"},"source":["# Note: since importing slides from Professor's Google API does not work sometimes,\n","# we downloaded the images to 'locally' in Google Drive\n","directory = '/content/drive/MyDrive/Applied_Deep_Learning_Project/slides_local/'\n","\n","biopsy_path_list = []\n","mask_path_list = []\n","\n","for file in os.scandir(directory):\n","  \n","  if (\"tif\" in file.name):\n","    if (\"mask\" in file.name):\n","      mask_path_list.append(file.path) \n","  \n","    else:\n","      biopsy_path_list.append(file.path)\n","\n","# # Automatically removes images that are not both exists in image and masks\n","biopsy_path_list,mask_path_list = balance_imgs(biopsy_path_list,mask_path_list)\n","\n","biopsy_path_list.sort()\n","mask_path_list.sort()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tissue image length: 22\n","Mask image length: 21\n","Removed image number 038 since there is no corresponding biopsy image\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2_v5SW89zeEn"},"source":["####  Since the number of biopsy and mask images are different, image number 38 got deleted from the initially given sample. Also, we removed some other biopsy/mask images that do not have cancer percentage high enough.<br>"]},{"cell_type":"code","metadata":{"id":"AZSBSo79-xC1"},"source":["def extract_img_numbers(string_list):\r\n","  int_list = []\r\n","  for str in string_list:\r\n","    img_num = re.findall(r'\\d+', str)[0]\r\n","    int_list.append(int(img_num))\r\n","\r\n","  return sorted(int_list)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KNw3JeRK7JG0"},"source":["### Decided to use 9 images for train set, 3 for valid set, and 3 for test set"]},{"cell_type":"code","metadata":{"id":"pFDB1F-KOAG9"},"source":["train_index = [2,4,5,6,7,15,17,19,20]\n","# test_index = [11,13,16]\n","test_index = [16]\n","valid_index = [0,18,12]\n","\n","biopsy_train = [biopsy_path_list[i] for i in train_index]\n","biopsy_test = [biopsy_path_list[i] for i in test_index]\n","biopsy_valid = [biopsy_path_list[i] for i in valid_index]\n","mask_train = [mask_path_list[i] for i in train_index]\n","mask_test = [mask_path_list[i] for i in test_index]\n","mask_valid = [mask_path_list[i] for i in valid_index]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vT75LDYv_7Ee","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608274028711,"user_tz":300,"elapsed":8271,"user":{"displayName":"Hyuk Joon Kwon","photoUrl":"","userId":"09809686230726823589"}},"outputId":"95017deb-c4f6-448f-cf10-64b57f78e6a0"},"source":["print(\"Total num of samples used: \", len(biopsy_train) + len(biopsy_valid) +len(biopsy_test))\r\n","print(\"Each train/val/test set length: \\n biopsy -> train: {} val: {} test: {} \\n Mask -> train: {} val: {} test: {}\".format(\r\n","     len(biopsy_train), len(biopsy_valid), len(biopsy_test), \r\n","     len(mask_train), len(mask_valid), len(mask_test)  ))\r\n","\r\n","print()\r\n","print(\"Biopsy img number in train set: {}\".format(extract_img_numbers(biopsy_train)))\r\n","print(\"Biopsy img number in valid set: {}\".format(extract_img_numbers(biopsy_valid)))\r\n","print(\"Biopsy img number in test set: {}\".format(extract_img_numbers(biopsy_test)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total num of samples used:  13\n","Each train/val/test set length: \n"," biopsy -> train: 9 val: 3 test: 1 \n"," Mask -> train: 9 val: 3 test: 1\n","\n","Biopsy img number in train set: [5, 16, 19, 23, 31, 84, 94, 101, 110]\n","Biopsy img number in valid set: [1, 75, 96]\n","Biopsy img number in test set: [91]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"D595-G-g-jYT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608274028711,"user_tz":300,"elapsed":8267,"user":{"displayName":"Hyuk Joon Kwon","photoUrl":"","userId":"09809686230726823589"}},"outputId":"d1c392b5-7dad-4eef-a3a8-82166d999d29"},"source":["#Getting the slides using each of the train/test/valid directory\n","# biopsy_slides_train = [get_open_slide(x,y,True) for x,y in zip(biopsy_train, mask_train)] # Ture -> reads tumor img, False -> reads masks image\n","# mask_slides_train = [get_open_slide(x,y,False) for x,y in zip(biopsy_train, mask_train)]\n","\n","# biopsy_slides_valid = [get_open_slide(x,y,True) for x,y in zip(biopsy_valid, mask_valid)]\n","# mask_slides_valid = [get_open_slide(x,y,False) for x,y in zip(biopsy_valid, mask_valid)]\n","\n","biopsy_slides_test = [get_open_slide(x,y,True) for x,y in zip(biopsy_test, mask_test)]\n","mask_slides_test = [get_open_slide(x,y,False) for x,y in zip(biopsy_test, mask_test)]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Read WSI from /content/drive/MyDrive/Applied_Deep_Learning_Project/slides_local/tumor_091.tif with width: 61440, height: 53760\n","Read tumor mask from /content/drive/MyDrive/Applied_Deep_Learning_Project/slides_local/tumor_091_mask.tif\n","Slide includes %d levels 8\n","Level 0, dimensions: (61440, 53760) downsample factor 1\n","Level 1, dimensions: (30720, 26880) downsample factor 2\n","Level 2, dimensions: (15360, 13440) downsample factor 4\n","Level 3, dimensions: (7680, 6720) downsample factor 8\n","Level 4, dimensions: (3840, 3360) downsample factor 16\n","Level 5, dimensions: (1920, 1680) downsample factor 32\n","Level 6, dimensions: (960, 840) downsample factor 64\n","Level 7, dimensions: (480, 420) downsample factor 128\n","Read WSI from /content/drive/MyDrive/Applied_Deep_Learning_Project/slides_local/tumor_091.tif with width: 61440, height: 53760\n","Read tumor mask from /content/drive/MyDrive/Applied_Deep_Learning_Project/slides_local/tumor_091_mask.tif\n","Slide includes %d levels 8\n","Level 0, dimensions: (61440, 53760) downsample factor 1\n","Level 1, dimensions: (30720, 26880) downsample factor 2\n","Level 2, dimensions: (15360, 13440) downsample factor 4\n","Level 3, dimensions: (7680, 6720) downsample factor 8\n","Level 4, dimensions: (3840, 3360) downsample factor 16\n","Level 5, dimensions: (1920, 1680) downsample factor 32\n","Level 6, dimensions: (960, 840) downsample factor 64\n","Level 7, dimensions: (480, 420) downsample factor 128\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qVbZ0Z9lxozc"},"source":["#### Use filter to facilitate training proces\n","##### i.e. regions where tissue is -> where grey value is lower than 0.8"]},{"cell_type":"code","metadata":{"id":"OoiysdHsxoap"},"source":["# As mentioned in class, we can improve efficiency by ignoring non-tissue areas \n","# of the slide. We'll find these by looking for all gray regions.\n","def find_tissue_pixels(image, intensity=0.8):\n","    im_gray = rgb2gray(image)\n","    assert im_gray.shape == (image.shape[0], image.shape[1])\n","    indices = np.where(im_gray <= intensity)\n","    return len(indices[0]) # i.e. return length of number of pixels that have lower intensity\n","\n","# A modified version of the finding tissue pixel \n","# Returns True if area of present biopsy image is above the threshold, min_percentage,\n","def pass_tissue_percentage(slide_image, tissue_p_min, w, h):\n","  tissue_pixels = find_tissue_pixels(slide_image)\n","  percent_tissue = tissue_pixels/ float(w*h) * 100\n","\n","  if percent_tissue > tissue_p_min:\n","    # print(\"----------------\",(\"%.2f\" % round(percent_tissue, 2)))\n","    return True\n","  else:\n","    return False\n","\n","# test whether a patch has cancer or not\n","def has_cancer(patch):\n","  return (sum(sum(chain.from_iterable(patch))) != 0) * 1  #i.e. returns 1 if a patch has a caner, else 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7xCxrTuWsT9S"},"source":["def downsample_factor(slide, level):\n","\n","  return (int(slide.level_downsamples[level]))\n","\n","\n","# Transforms a coordinate associated with a current zoom level\n","def transform_coord(slide, input_coord, base_level, current_level, patch_size):\n","\n","  base_lev_factor = downsample_factor(slide,base_level)\n","  current_lev_factor = downsample_factor(slide,current_level)\n","\n","  new_coord = input_coord * base_lev_factor - int(patch_size/2) * current_lev_factor\n","  return new_coord\n","\n","\n","# Reads slide according to the transformed coordinate\n","def read_zoom_slide(slide, x_input, y_input, base_lev, current_lev, patch_size):\n","\n","  slide = read_slide(slide, \n","                     transform_coord(slide, x_input, base_lev, current_lev, patch_size ),\n","                     transform_coord(slide, y_input, base_lev, current_lev, patch_size ),\n","                     level = current_lev, width = patch_size, height = patch_size)\n","  return slide\n","\n","# Computes a relative magnification difference between two zoom levels\n","def compute_relative_mag(base_lev, current_lev):\n","\n","  result = 2 ** (base_lev - current_lev)  # i.e. base_lev = 7, current_lev =5, then current level is zoomed 4 times more\n","  return result\n","\n","\n","# Computes Rectangle with appropriate coordinates in the current zoom level,\n","# with relative to the base level (most zoomed out level) \n","def compute_rectangle(relative_magnification, patch_size = 299, center_size = 128):\n","\n","  return Rectangle((int(patch_size / 2) - int(center_size / (2 ** (relative_magnification + 1))),\n","                  int(patch_size / 2) - int(center_size / (2 ** (relative_magnification + 1)))),\n","                  center_size / (2**relative_magnification),\n","                  center_size  / (2**relative_magnification),\n","                  facecolor='none', edgecolor='red')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BzqpP5_cMk-3"},"source":["def print_patches(patch_list, base_l, current_l):\n","  number_patches = len(patch_list)\n","  fig, axes = plt.subplots(nrows=1, ncols=number_patches, figsize=(15,10))\n","  for idx, slide in enumerate(patch_list):\n","    axes[idx].imshow(slide)\n","    axes[idx].add_patch(compute_rectangle(idx))\n","\n","def print_masks(patch_list, base_l, current_l):\n","  number_patches = len(patch_list)\n","  fig, axes = plt.subplots(nrows=1, ncols=number_patches, figsize=(15,10))\n","  for idx, slide in enumerate(patch_list):\n","    axes[idx].imshow(slide[:,:,0])\n","    axes[idx].add_patch(compute_rectangle(idx))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JOIZMnUOyzgA"},"source":["  def index_values_l( mylist, value): # print indices in a list where value is repetitive\r\n","      return [i for i,x in enumerate(mylist) if x==value]\r\n","\r\n","  def undersampling_labels(label_list):\r\n","    indicies_1 = index_values_l(label_list,1)\r\n","    indicies_0 = index_values_l(label_list,0)\r\n","    # print(\"1: \",indicies_1, \"length: \", len(indicies_1 ))\r\n","    # print(\"0: \",indicies_0,  \"length: \", len(indicies_0 ))\r\n","    # print()\r\n","\r\n","    temp = list(zip(indicies_1, indicies_0)) \r\n","    np.random.shuffle(temp) \r\n","    shuffle_i_1, shuffle_i_0 = zip(*temp) \r\n","\r\n","    min_length = min(len(indicies_1),len(indicies_0))\r\n","    # print(\"min_length: \", min_length)\r\n","    # print()\r\n","    # print(\"index of 1: \", indicies_1[:min_length])\r\n","    # print(\"index of 0: \", indicies_0[:min_length])\r\n","\r\n","    balanced_index = sorted(indicies_1[:min_length] + indicies_0[:min_length]) # i.e. sorted\r\n","    return balanced_index\r\n","\r\n","\r\n","# #testing\r\n","# abcd = [1,0,0,0,0,0,1,1,0,0,0,0,0,0]\r\n","# undersampling_labels(abcd)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3N1BAAfND0fB"},"source":["def create_patches(biopsy_slide, mask_slide, lowest_level = 5, highest_level = 7, apply_filter = False, stride = 100, tissue_p_min = 50):\n","\n","  p_size = 299  # patch size\n","  stride = stride  # need to chnage (here, for zoom level 5, which is lowest level)\n","\n","  tissue_p_min = tissue_p_min  # i.e. treshold of minimum percentage of tissue converaged in percentage\n","                # i.e. if patch has more than 10% of tissue pixels, then take it\n","  biopsy_list = []\n","  label_list = []\n","\n","  # Starting center coordinates\n","  x = int(p_size/2) # i.e. 150\n","  y = int(p_size/2)\n","\n","  biopsy_w_high = biopsy_slide.level_dimensions[highest_level][0]\n","  biopsy_h_high = biopsy_slide.level_dimensions[highest_level][1]\n","\n","  biopsy_w_low = biopsy_slide.level_dimensions[lowest_level][0]\n","  biopsy_h_low = biopsy_slide.level_dimensions[lowest_level][1]\n","\n","  # print(biopsy_w_high, biopsy_w_high * slide.level_downsamples[base])\n","\n","  biopsy_list_batch = []\n","  \n","  run = True\n","\n","  global patch_counter\n","  patch_counter=0\n","  patch_x_counter=0\n","  patch_y_counter=0\n","\n","  print(\"lowest dim shape:\",biopsy_w_low, \" \", biopsy_h_low )\n","\n","  # print(\"Applying filtering? : -> \", apply_filter)\n","\n","  while(run):\n","    # print(\"Coordinate: \" + str(x) + \" \" + str(y))\n","     \n","\n","    if (y < biopsy_h_high  - 299//2):  # i.e. slide based on highest level since it is most zoomed out version\n","      if (x < biopsy_w_high - 299//2):\n","        biopsy_list_batch = []\n","\n","        slide_image_temp = read_zoom_slide(biopsy_slide, x, y, highest_level, lowest_level, p_size)\n","\n","        if apply_filter: # i.e. apply filtering only for training set, s\n","          # print(\"Applying tissue filtering\")\n","          if pass_tissue_percentage(slide_image_temp, tissue_p_min, p_size, p_size): # i.e. only using if lowest label passes the tissue test\n","\n","            for current_level in range(lowest_level, highest_level + 1):\n","              slide_image = read_zoom_slide(biopsy_slide, x, y, highest_level, current_level, p_size)\n","              mask_image = read_zoom_slide(mask_slide, x, y, highest_level, current_level, p_size)\n","\n","              biopsy_list_batch.append(slide_image)\n","\n","              mask_image = read_zoom_slide(mask_slide, x, y, highest_level, current_level, p_size)\n","              if (current_level == lowest_level): # Do it with only the first image\n","                # print(\"pass_cord: \" + str(x) + \" \" + str(y) + \" \" + str(has_cancer(mask_image)), end=', ', flush=True)\n","                label_list.append(has_cancer(mask_image))\n","                                        \n","              else:\n","                continue;\n","\n","        else:  # do not apply tissue filtering for valid, test set\n","          # print(\"Not applying filtering for training set\")\n","          for current_level in range(lowest_level, highest_level + 1):\n","            slide_image = read_zoom_slide(biopsy_slide, x, y, highest_level, current_level, p_size)\n","            mask_image = read_zoom_slide(mask_slide, x, y, highest_level, current_level, p_size)\n","\n","            biopsy_list_batch.append(slide_image)\n","\n","            mask_image = read_zoom_slide(mask_slide, x, y, highest_level, current_level, p_size)\n","            if (current_level == lowest_level): # Do it with only the first image\n","              # print(\"pass_cord: \" + str(x) + \" \" + str(y) + \" \" + str(has_cancer(mask_image)), end=', ', flush=True)\n","              label_list.append(has_cancer(mask_image))\n","                                      \n","            else:\n","              continue;\n","        # print_patches(biopsy_list_batch, highest_level, current_level)\n","        # print_masks(mask_list_batch, highest_level, current_level)\n","\n","        if len(biopsy_list_batch) !=0:\n","          biopsy_list.append(biopsy_list_batch)\n","  \n","        patch_counter += 1\n","        patch_x_counter += 1\n","        x = x + int(stride / biopsy_slide.level_downsamples[highest_level] *  biopsy_slide.level_downsamples[lowest_level])\n","        \n","      else: # i.e. progress toward next 'row' below\n","        x = int(p_size/2) \n","        y = y + int(stride / biopsy_slide.level_downsamples[highest_level] *  biopsy_slide.level_downsamples[lowest_level])\n","      \n","        patch_y_counter += 1\n","        if (y < biopsy_h_high  - 299//2):\n","          patch_x_counter = 0\n","\n","    else: \n","      run = False  \n","\n","  # i.e. undersampled labels into a balanced set (i.e.same )\n","  # balanced_index = undersampling_labels(label_list)\n","  # label_list = [label_list[i] for i in balanced_index]\n","  # biopsy_list = [biopsy_list[i] for i in balanced_index]  \n","\n","  assert len(biopsy_list) == len(label_list)\n","  print(\"tissue filtering percentage used: \", \"{:.2f}\".format(len(biopsy_list)/patch_counter*100), \"%\")\n","\n","  print(\"num of patches per zoom lev:\", len(label_list))\n","  if apply_filter: # i.e. for training set\n","    return biopsy_list, label_list\n","  \n","  else: # i.e. for test set, also returning patches_num\n","    patches_num = [patch_counter, patch_x_counter, patch_y_counter]\n","    print(\"patches num: \", patches_num)\n","    \n","    return biopsy_list, label_list, patches_num \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d249BrB9kg5q"},"source":["### Flatten and put train and validation set into Numpy array formats"]},{"cell_type":"code","metadata":{"id":"Is4BpB-vkFHM"},"source":["# flatten each biopsy patch into its zoom level\r\n","def flatten_biopsy_zoom(biopsy_patch):\r\n","  zoom1 = []\r\n","  zoom2 = []\r\n","  zoom3 = []\r\n","\r\n","  for biopsy in biopsy_patch:\r\n","    # print(np.array(biopsy).shape)\r\n","    for patch_img in biopsy:\r\n","      zoom1.append(patch_img[0])\r\n","      zoom2.append(patch_img[1])\r\n","      zoom3.append(patch_img[2])\r\n","\r\n","  assert np.array(zoom1).shape == np.array(zoom2).shape\r\n","  return  np.array(zoom1),  np.array(zoom2), np.array(zoom3)\r\n","\r\n","# flatten label\r\n","def flatten_label(label_list):\r\n","  label = []\r\n","  for image in label_list:\r\n","    for i in image:\r\n","      label.append(i)\r\n","\r\n","  print(\"Total num of patches per zoom level\", len(label))\r\n","  return np.array(label)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6t0lXiS5E6v7"},"source":["#### Save Test set into Numpy format"]},{"cell_type":"code","metadata":{"id":"XvTGi-dc66ws"},"source":["# files_list = os.listdir(saving_dir)\r\n","\r\n","# if len(files_list) != 0:\r\n","#   for file in files_list: #i.e. delete old files each time\r\n","#     os.remove(os.path.join(saving_dir,file))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VCqTbi3joMC0","executionInfo":{"status":"ok","timestamp":1608274273650,"user_tz":300,"elapsed":253175,"user":{"displayName":"Hyuk Joon Kwon","photoUrl":"","userId":"09809686230726823589"}},"outputId":"1766e672-0b6a-4fe5-b047-5a74e0c7104f"},"source":["print(\"Saving test index number: \", test_index[0])\n","\n","result_test =[create_patches(slide, mask, lowest_level = lowest_level, highest_level = highest_level, \n","                              apply_filter = False, stride = test_stride, \n","                              tissue_p_min = tissue_p_min) for slide,mask in zip(biopsy_slides_test, mask_slides_test)]\n","biopsy_patch_test = [item[0] for item in result_test]\n","label_test =[item[1] for item in result_test]\n","patches_num_test = [item[2] for item in result_test] # i.e. list of sublist, where sublist is [patches_num, x_num, y_num]\n","                                                              # patch_counter, patch_x_counter, patch_y_counter\n","\n","zoom1_test, zoom2_test, zoom3_test= flatten_biopsy_zoom(biopsy_patch_test)\n","label_test = flatten_label(label_test)\n","\n","# saving files in Numpy formats\n","print()\n","np.save(os.path.join(saving_dir,'patches_num_test_' + str(test_index[0]) +'.npy'),patches_num_test)\n","np.save(os.path.join(saving_dir,'zoom1_test_' + str(test_index[0]) + '.npy'),zoom1_test)\n","np.save(os.path.join(saving_dir,'zoom2_test_' + str(test_index[0]) + '.npy'),zoom2_test)\n","np.save(os.path.join(saving_dir,'zoom3_test_' + str(test_index[0]) + '.npy'),zoom3_test)\n","np.save(os.path.join(saving_dir,'label_test_' + str(test_index[0]) + '.npy'),label_test)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Saving test index number:  16\n","lowest dim shape: 15360   13440\n","tissue filtering percentage used:  100.00 %\n","num of patches per zoom lev: 2016\n","patches num:  [2016, 48, 42]\n","Total num of patches per zoom level 2016\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"34g2-fh5kh7g"},"source":[""],"execution_count":null,"outputs":[]}]}